{"cells":[{"cell_type":"markdown","metadata":{"id":"1-1D-NcYYON-"},"source":["## CS 445 Final Project: Model for Recognizing Emotions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XHKdwsfjdmWx","outputId":"0eab3609-c172-40ce-b20d-aff43751b586"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torchvision in /Applications/anaconda3/lib/python3.8/site-packages (0.8.2)\n","Requirement already satisfied: numpy in /Applications/anaconda3/lib/python3.8/site-packages (from torchvision) (1.20.1)\n","Requirement already satisfied: torch in /Applications/anaconda3/lib/python3.8/site-packages (from torchvision) (1.7.1)\n","Requirement already satisfied: pillow>=4.1.1 in /Applications/anaconda3/lib/python3.8/site-packages (from torchvision) (8.2.0)\n","Requirement already satisfied: typing_extensions in /Applications/anaconda3/lib/python3.8/site-packages (from torch->torchvision) (3.7.4.3)\n"]}],"source":["!pip install torchvision"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bkgFvVZUYS5K"},"outputs":[],"source":["import numpy as np\n","import cv2\n","import scipy.linalg\n","import math\n","import os\n","%matplotlib inline\n","from matplotlib import pyplot as plt\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optimdef\n","from skimage import io, transform\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JFK2v3isFYPu","outputId":"f6cd564c-8e66-4c5b-d530-2f5a9ce631a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting gif2numpy\n","  Downloading gif2numpy-1.3.zip (10 kB)\n","Requirement already satisfied: numpy in /Users/rushalbutala/opt/anaconda3/lib/python3.9/site-packages (from gif2numpy) (1.22.3)\n","Collecting kaitaistruct\n","  Downloading kaitaistruct-0.9.tar.gz (5.5 kB)\n","Building wheels for collected packages: gif2numpy, kaitaistruct\n","  Building wheel for gif2numpy (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for gif2numpy: filename=gif2numpy-1.3-py2.py3-none-any.whl size=8391 sha256=3b5cd07faab2e069eef75f630091fd37fd5e9fc28d3159680a2d06e35d39c37c\n","  Stored in directory: /Users/rushalbutala/Library/Caches/pip/wheels/e0/9a/24/90d9287b2472b132fed8e91d85b3acd6ec39c9420d512120d6\n","  Building wheel for kaitaistruct (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for kaitaistruct: filename=kaitaistruct-0.9-py2.py3-none-any.whl size=5512 sha256=2dd496bfefd05f7e5e52e58e9759590db0988914929a7d321903ffc2f395b0d6\n","  Stored in directory: /Users/rushalbutala/Library/Caches/pip/wheels/da/13/6a/a51f1f909ab2ea9ac43db5616577860868944de1d45d2f098f\n","Successfully built gif2numpy kaitaistruct\n","Installing collected packages: kaitaistruct, gif2numpy\n","Successfully installed gif2numpy-1.3 kaitaistruct-0.9\n","Collecting opencv-python\n","  Using cached opencv_python-4.5.5.64-cp36-abi3-macosx_10_15_x86_64.whl (46.3 MB)\n","Requirement already satisfied: numpy>=1.19.3 in /Users/rushalbutala/opt/anaconda3/lib/python3.9/site-packages (from opencv-python) (1.22.3)\n","Installing collected packages: opencv-python\n","Successfully installed opencv-python-4.5.5.64\n"]}],"source":["# # added by heather\n","!pip install gif2numpy\n","!pip install opencv-python\n","# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZFwSUKbYTF3"},"outputs":[],"source":["import gif2numpy\n","import os\n","import random\n","# base_dir= \"/Users/charl/Downloads/yalefaces/yalefaces/\"\n","base_dir= \"/Users/gauss/Downloads/Final Submission-2/yalefaces/new/\"\n","\n","dirs = os.listdir(base_dir)\n","N = len(dirs)\n","images = np.zeros(N)\n","image_labels = np.zeros(N)\n","\n","for i in range(N):\n","    if dirs[i].endswith(\"happy.gif\"):\n","        image_labels[i] = 1\n","    elif dirs[i].endswith(\"sad.gif\"):\n","        image_labels[i] = 2\n","\n","images = np.zeros((N, 243, 320, 3))\n","# This would print all the files and directories\n","idx = 0\n","for file in dirs:\n","    np_images, extensions, image_specs = gif2numpy.convert(base_dir+file)\n","    # print(\"type of image:\", image, type(np_images[0]))\n","    # cv2_imshow(np_images[0])\n","    images[idx] = np_images[0]\n","    idx += 1\n","\n","# images = images[:, :, :, 0]\n","# cv2.destroyWindow(\"np_image\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k8uRGSTcdmWz","outputId":"7ffdc224-01af-4386-a63c-050819b8535f"},"outputs":[{"name":"stdout","output_type":"stream","text":["(132, 170, 170)\n"]}],"source":["# recognize faces\n","# square images\n","# base code from this source: https://towardsdatascience.com/face-detection-in-2-minutes-using-opencv-python-90f89d7c0f81 \n","\n","# Load the cascade\n","face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n"," \n","# Read the input images by accessing images[i] (from previous code cell)\n","\n","dimensions = []\n","n = 0\n","gray_images = np.zeros((N, 243, 320))\n","\n","for image in images :\n","    img = np.uint8(image)\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    gray_images[n] = gray\n","\n","      # Detect faces\n","    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n","      # print(faces.shape)\n","      # Draw rectangle around the faces\n","    dimensions.append(faces)\n","    n += 1\n","\n","maxi = 0\n","\n","for faces in dimensions:\n","    for (x, y, w, h) in faces:\n","        if w > maxi:\n","            maxi = w\n","            \n","        if h > maxi:\n","            maxi = h\n","\n","final_faces = np.zeros((N, maxi, maxi))   \n","i = 0\n","\n","for faces in dimensions:\n","    for (x, y, w, h) in faces:\n","        final_faces[i] = cv2.resize(gray_images[i, y: y+ h,x : x + w], (maxi, maxi))\n","    \n","    i += 1\n","      # Display the output\n","      #  train_images[n]=img\n","\n","\n","size = int(N*0.8)\n","\n","train_indices = random.sample(range(N), size)\n","test_indices = np.setdiff1d(range(N), train_indices)\n","\n","\n","#   idx = random.randint(0, len(gray_images))\n","#print(train_indices,test_indices)\n","train_images = final_faces[train_indices].astype(np.uint8)\n","test_images = final_faces[test_indices].astype(np.uint8)\n","train_labels = image_labels[train_indices]\n","test_labels = image_labels[test_indices]\n","print(train_images.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K98A_Dur1Lq5"},"outputs":[],"source":["class FaceDataset(Dataset):\n","    def __init__(self, images, labels, transform=None):\n","        self.images = images.astype(np.float32)\n","        print(self.images.shape)\n","        self.labels = labels\n","        self.transform = transform\n","        \n","    def __len__(self):\n","        return len(self.images)\n","    \n","    def __getitem__(self, idx):\n","        return self.images[idx], self.labels[idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dUIkvd088H0t","outputId":"8786c456-1b83-4c0a-897a-4eebaf59eae4"},"outputs":[{"name":"stdout","output_type":"stream","text":["(132, 170, 170)\n","(34, 170, 170)\n"]}],"source":["classes = list(str(i) for i in range(3))\n","batch_size = 3\n","\n","\n","trainset = FaceDataset(images = train_images, labels = train_labels)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=0)\n","\n","testset = FaceDataset(images = test_images, labels = test_labels)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=False, num_workers=0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k6e3jhO9dmW1","outputId":"38710657-6c07-4766-b78e-b72fa1fad3d7"},"outputs":[{"data":{"text/plain":["25"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["_, height, width = train_images.shape\n","kernel_size = 5\n","pool = 5\n","def calculate_shape(height, width, kernel_size, pool):\n","    in_height = int(((((height-kernel_size)+1)/pool-kernel_size)+1)/pool)\n","    in_width = int(((((width-kernel_size)+1)/pool-kernel_size)+1)/pool)\n","    return in_height * in_width\n","calculate_shape(height, width, kernel_size, pool)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IewCPogV4J-D"},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(1, 6, kernel_size) #gray scale image, single input channel\n","        self.pool = nn.MaxPool2d(pool, pool)\n","        self.conv2 = nn.Conv2d(6, 16, kernel_size)\n","        self.fc1 = nn.Linear(16*calculate_shape(height, width, kernel_size, pool), 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 3)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        \n","        return x\n","\n","\n","net = Net()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fKtB2Uu-riJ4"},"outputs":[],"source":["import torch.optim as optim\n","\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EIAODwfYYTaH","outputId":"2a27730b-f05a-40ab-91ed-11aac23160d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","Finished Training\n"]}],"source":["# test model w/ new dataset\n","for epoch in range(10):  # loop over the dataset multiple times\n","        print(epoch)\n","        running_loss = 0.0\n","        \n","        for i, data in enumerate(trainloader, 0):\n","            # get the inputs; data is a list of [inputs, labels]\n","            inputs, labels = data\n","            labels = labels.type(torch.LongTensor)\n","            \n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = net(inputs.unsqueeze(1)) # add one dimension to grayscale image\n","            loss = loss_fn(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # print statistics\n","            running_loss += loss.item()\n","        \n","            if i % 2000 == 1999:    # print every 2000 mini-batches\n","                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n","                running_loss = 0.0\n","\n","print('Finished Training')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nqx-y4gFYTlG"},"outputs":[],"source":["# output results & accuracy of model\n","PATH = './face_net.pth'\n","torch.save(net.state_dict(), PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FvEICq7_1Lq8","outputId":"5becd47b-72e1-4ffe-e71a-076d376b1b83"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of the network on the 10000 test images: 91 %\n"]}],"source":["correct = 0\n","total = 0\n","# since we're not training, we don't need to calculate the gradients for our outputs\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","        # calculate outputs by running images through the network\n","        outputs = net(images.unsqueeze(1))\n","        # the class with the highest energy is what we choose as prediction\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DIb6X8jkdmW4"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"91% with conv2d and results.ipynb","provenance":[]},"interpreter":{"hash":"cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"},"kernelspec":{"display_name":"Python 3.8.8 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}
